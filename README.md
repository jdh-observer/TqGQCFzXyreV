# Speaking with our Sources: Large Language Models as Tools for Historical Research

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jdh-observer/TqGQCFzXyreV/main?filepath=article.ipynb)


This article investigates the transformative potential of Large Language Models (LLMs) in historical research, proposing a dual methodological framework to navigate the "age of abundance" in digital archives. We contrast Retrieval Augmented Generation (RAG) with fine-tuning to demonstrate how LLMs serve distinct epistemological functions: as semantic search engines and as simulators of historical discourse. Using corpora of American Presidential speeches, our RAG system illustrates the power of vector-based retrieval to surface relevant content without relying on exact keyword matches, thereby enhancing archival discovery while maintaining clear source provenance. Conversely, by fine-tuning models on presidential Q&A datasets, we show how LLMs can internalise specific historical voices, offering a novel mechanism to study discourse through generative simulation. We argue these approaches are complementary, where RAG prioritises the verifiable retrieval of information, fine-tuning offers a hermeneutic tool for modelling the probabilistic logic of historical language. Ultimately, this study posits that the emergence of LLMs necessitates a disciplinary pivot, in which historians will increasingly blend traditional close reading with computational "distant reading" to better interrogate the vast, rapidly accumulating, unstructured data of the past.